#!/usr/bin/env python3
"""
æ™ºèƒ½æ¨¡å‹é€‰æ‹©ç³»ç»Ÿ
ä¼˜å…ˆä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹ï¼Œå¤±è´¥æ—¶å›é€€åˆ°API
"""

import requests
import json
import time
import logging
from typing import Optional, Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SmartModelSelector:
    """æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨"""
    
    def __init__(self):
        self.config = self.load_config()
        self.local_model_available = self.check_local_model()
        
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open('local_model_config.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            # é»˜è®¤é…ç½®
            return {
                "model": "ollama/qwen2.5:1.5b-instruct",
                "apiBase": "http://localhost:11434/v1",
                "fallbackModel": "deepseek/deepseek-chat",
                "maxTokens": 2048,
                "temperature": 0.7,
                "timeout": 30000,
                "retryCount": 2
            }
    
    def check_local_model(self) -> bool:
        """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(
                "http://localhost:11434/api/tags",
                timeout=5
            )
            if response.status_code == 200:
                models = response.json().get("models", [])
                target_model = self.config["model"].split("/")[-1]
                for model in models:
                    if target_model in model.get("name", ""):
                        logger.info(f"âœ… æœ¬åœ°æ¨¡å‹å¯ç”¨: {model.get('name')}")
                        return True
                logger.warning(f"âš ï¸ æœ¬åœ°æ¨¡å‹ {target_model} æœªæ‰¾åˆ°")
            return False
        except Exception as e:
            logger.warning(f"âŒ æœ¬åœ°æ¨¡å‹æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def generate_with_local(self, prompt: str) -> Optional[str]:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆ"""
        if not self.local_model_available:
            return None
        
        try:
            payload = {
                "model": self.config["model"].split("/")[-1],
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": self.config.get("maxTokens", 2048),
                    "temperature": self.config.get("temperature", 0.7)
                }
            }
            
            response = requests.post(
                f"{self.config['apiBase'].replace('/v1', '')}/api/generate",
                json=payload,
                timeout=self.config.get("timeout", 30)
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            else:
                logger.error(f"æœ¬åœ°æ¨¡å‹è¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
                
        except requests.exceptions.Timeout:
            logger.warning("æœ¬åœ°æ¨¡å‹è¯·æ±‚è¶…æ—¶")
            return None
        except Exception as e:
            logger.error(f"æœ¬åœ°æ¨¡å‹ç”Ÿæˆé”™è¯¯: {e}")
            return None
    
    def generate_with_fallback(self, prompt: str) -> str:
        """æ™ºèƒ½ç”Ÿæˆï¼šå…ˆå°è¯•æœ¬åœ°ï¼Œå¤±è´¥åˆ™ä½¿ç”¨å›é€€"""
        logger.info(f"ğŸ“ å¤„ç†è¯·æ±‚: {prompt[:50]}...")
        
        # å…ˆå°è¯•æœ¬åœ°æ¨¡å‹
        if self.local_model_available:
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹...")
            local_result = self.generate_with_local(prompt)
            if local_result:
                logger.info("âœ… æœ¬åœ°æ¨¡å‹ç”ŸæˆæˆåŠŸ")
                return local_result
            else:
                logger.warning("âš ï¸ æœ¬åœ°æ¨¡å‹å¤±è´¥ï¼Œåˆ‡æ¢åˆ°API")
        
        # ä½¿ç”¨å›é€€æ¨¡å‹ï¼ˆè¿™é‡Œéœ€è¦å®é™…çš„APIè°ƒç”¨ï¼‰
        logger.info(f"ğŸ”„ ä½¿ç”¨å›é€€æ¨¡å‹: {self.config['fallbackModel']}")
        # å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ç›¸åº”çš„API
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿå“åº”
        return f"[ä½¿ç”¨APIæ¨¡å‹ {self.config['fallbackModel']}] è¿™æ˜¯æ¨¡æ‹Ÿå“åº”ã€‚å®é™…ä¼šè°ƒç”¨ç›¸åº”APIã€‚"
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = {
            "local_available": self.local_model_available,
            "local_model": self.config["model"],
            "fallback_model": self.config["fallbackModel"],
            "strategy": "local-first"
        }
        
        if self.local_model_available:
            try:
                response = requests.get(
                    "http://localhost:11434/api/tags",
                    timeout=5
                )
                if response.status_code == 200:
                    models = response.json().get("models", [])
                    if models:
                        info["local_model_details"] = models[0]
            except:
                pass
        
        return info

def test_model_selector():
    """æµ‹è¯•æ¨¡å‹é€‰æ‹©å™¨"""
    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½æ¨¡å‹é€‰æ‹©ç³»ç»Ÿ...")
    
    selector = SmartModelSelector()
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    info = selector.get_model_info()
    print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    print(f"   æœ¬åœ°æ¨¡å‹å¯ç”¨: {info['local_available']}")
    print(f"   æœ¬åœ°æ¨¡å‹: {info['local_model']}")
    print(f"   å›é€€æ¨¡å‹: {info['fallback_model']}")
    print(f"   ç­–ç•¥: {info['strategy']}")
    
    # æµ‹è¯•ç”Ÿæˆ
    test_prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
    print(f"\nğŸ“ æµ‹è¯•æç¤º: {test_prompt}")
    
    result = selector.generate_with_fallback(test_prompt)
    print(f"ğŸ“„ ç”Ÿæˆç»“æœ: {result}")
    
    return selector

if __name__ == "__main__":
    selector = test_model_selector()
    print("\nğŸ‰ æ™ºèƒ½æ¨¡å‹é€‰æ‹©ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")